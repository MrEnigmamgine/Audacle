import os, re, json
import copy
import pandas as pd
import torch
import whisper
from typing import List
from .vad import VAD
from .asr import ASR
from .alignment import Aligner

SR=16000

# IO functions
def cache_json(data: object, path: str):
    with open(path, 'w') as f:
        json.dump(data, f)


def load_json(path):
    with open(path, 'r') as f:
        data = json.load(f)
    return data


def get_waveform(filepath: str):
    """Returns a tensor from an audio file."""
    wav = whisper.load_audio(filepath)
    return torch.from_numpy(wav)


# VAD handling
def new_vad_data(audio):
    vad = VAD()
    segments = vad.get_speech_timestamps(audio)
    return segments


def get_vad_data(path:str, waveform=None, refresh=False):
    json_path = f"{path}_vad.json"
    if os.path.exists(json_path) and not refresh:
        segments = load_json(json_path)
    else:
        if waveform == None:
            waveform = get_waveform(path)
        print('Calculating VAD segments.')
        segments = new_vad_data(waveform)
        cache_json(segments, json_path)
        print(f"Done. Data saved: '{json_path}'")
    return segments


def collect_chunks(wav: torch.Tensor, segments: List[dict]):
    """Edits a waveform to include only the segements in a list of segments."""
    chunks = []
    for i in segments:
        chunks.append(wav[i['start']: i['end']])
    return torch.cat(chunks)


def align_chunks(segments: List[dict]):
    """Returns a list of segments to align with the waveform generated by the collect chunks function."""
    chunks = []
    current_frame = 0
    for entry in segments:
        speech_length = entry['end'] - entry['start']
        end_frame = current_frame + speech_length
        chunks.append(
            {'start': current_frame,
             'end': end_frame}
            )
        current_frame = end_frame
    return chunks


def drop_chunks(tss: List[dict],
                wav: torch.Tensor):
    chunks = []
    cur_start = 0
    for i in tss:
        chunks.append((wav[cur_start: i['start']]))
        cur_start = i['end']
    return torch.cat(chunks)


def collect_and_align_chunks(wav: torch.Tensor, segments: List[dict]):
    new_wav = collect_chunks(wav, segments)
    new_segs = align_chunks(segments)
    return new_wav, new_segs


#ASR Handling
def new_asr_data(audio, verbose=False, model_name = 'medium.en'):
    asr = ASR(model_name)
    results = asr.transcribe(audio, verbose=verbose)
    return results['segments']


def get_asr_data(path, waveform=None, refresh=False, model_name = 'medium.en'):
    json_path = f"{path}_asr.json"
    if os.path.exists(json_path) and not refresh:
        asr_data = load_json(json_path)
    else:
        if waveform==None:
            waveform = get_waveform(path)
        wav = collect_chunks(waveform, get_vad_data(path, waveform=waveform))
        print('Transcribing ASR data')
        asr_data = new_asr_data(wav, model_name=model_name)
        cache_json(asr_data, json_path)
        print(f"Done. Data saved: '{json_path}'")
    return asr_data


def get_full_text(data, separator = ''):
    return separator.join([segment['text'] for segment in data])


# Alignment handling

def convert_timestamp_segments_to_frames(segments, sr=SR):
    df = pd.DataFrame(segments)
    df[['start','end']] = (df[['start','end']] * sr).round().astype(int)
    return df.to_dict(orient='records')


def convert_frames_segments_to_timestamp(segments, sr=SR):
    df = pd.DataFrame(segments)
    df[['start','end']] = (df[['start','end']] / sr)
    return df.to_dict(orient='records')


def new_alignment(asr_data, audio):
    aligner = Aligner()
    alignment_data = aligner.align(asr_data, audio)
    alignment_data = convert_timestamp_segments_to_frames(alignment_data['word_segments'])
    return alignment_data

def get_alignment_data(path, waveform=None, vad_data=None, asr_data=None, refresh=False):
    json_path = f"{path}_align.json"
    if os.path.exists(json_path) and not refresh:
        alignment_data = load_json(json_path)
    else:
        if waveform==None:
            waveform = get_waveform(path)
        if vad_data==None:
            vad_data = get_vad_data(path, waveform)
        if asr_data==None:
            asr_data = get_asr_data(path, waveform)
        print('Aligning ASR data')
        short_wav = collect_chunks(waveform, vad_data)
        alignment_data = new_alignment(asr_data, short_wav)
        cache_json(alignment_data, json_path)
        print(f"Done. Data saved: '{json_path}'")
    return alignment_data

def get_range_overlap_percent(parent: range, child: range) -> float:
    """Calculates the percentage of which the child's boundries fit within the parent's boundries."""
    olap = range(max(parent[0], child[0]), min(parent[-1], child[-1])+1)
    olap_percent = len(olap) / len(child)
    return olap_percent

def child_in_parent(parent, child, threshold=0.5) -> bool:
    s = range(parent['start'],parent['end'])
    w = range(child['start'],child['end'])
    return get_range_overlap_percent(s, w) >= threshold

def gather_children(parents, children):
    results = []
    for parent in parents:
        out = []
        for idx, child in enumerate(children):
            if child_in_parent(parent, child):
                out.append(child)
        copy = parent.copy()
        copy['children'] = out
        results.append(copy)
    return results

def migrate_children(source, target):
    """Copies child segments from source segment to the target segment and adjust timestamps to match."""
    out = copy.deepcopy(target)
    diff = target['start'] - source['start']
    out['children'] = copy.deepcopy(source['children'])
    for child in out['children']:
        child['start'] = child['start'] + diff
        child['end'] = child['end'] + diff
    return out

def batch_migrate_children(source, target):
    return [migrate_children(s,t) for s,t in zip(source,target)]

def get_srt_data(path, refresh=False):
    vad_data = get_vad_data(path, refresh=refresh)
    align_data = get_alignment_data(path, refresh=refresh)
    vad_short = align_chunks(vad_data)
    srt_short = gather_children(vad_short, align_data)
    srt_data = batch_migrate_children(srt_short, vad_data)
    for segment in srt_data:
        segment['text'] = get_full_text(segment['children'], ' ')
    return srt_data