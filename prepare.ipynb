{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re, json\n",
    "from IPython.display import Audio, display\n",
    "import torch\n",
    "import whisper\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import VAD, AVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a short file for testing if it doesn't exist.\n",
    "example_file = './data/en_example.wav'\n",
    "if not os.path.exists(example_file):\n",
    "    torch.hub.download_url_to_file('https://models.silero.ai/vad_models/en.wav', example_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing AVR data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4494/4494 [00:19<00:00, 229.30frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Data saved: './data/en_example.wav_avr.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import utils.prepare as prep\n",
    "\n",
    "_ = prep.get_avr_data(example_file, refresh=True, model_name='medium.en')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_json(data: object, path: str):\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Audio to VAD segments with cacheing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_waveform(filepath: str):\n",
    "    \"\"\"Returns a tensor from an audio file.\"\"\"\n",
    "    wav = whisper.load_audio(filepath)\n",
    "    return torch.from_numpy(wav)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_vad_data(audio):\n",
    "    vad = VAD()\n",
    "    segments = vad.get_speech_timestamps(audio)\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vad_data(path:str, waveform=None, refresh=False):\n",
    "    json_path = f\"{path}_vad.json\"\n",
    "    if os.path.exists(json_path) and not refresh:\n",
    "        segments = load_json(json_path)\n",
    "    else:\n",
    "        if waveform == None:\n",
    "            waveform = get_waveform(path)\n",
    "        print('Calculating VAD segments.')\n",
    "        segments = new_vad_data(waveform)\n",
    "        cache_json(segments, json_path)\n",
    "        print(f\"Done. Data saved: '{json_path}'\")\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "segs = get_vad_data(example_file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using VAD segments to extract speech only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_chunks(wav: torch.Tensor, segments: List[dict]):\n",
    "    \"\"\"Edits a waveform to include only the segements in a list of segments.\"\"\"\n",
    "    chunks = []\n",
    "    for i in segments:\n",
    "        chunks.append(wav[i['start']: i['end']])\n",
    "    return torch.cat(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_chunks(segments: List[dict]):\n",
    "    \"\"\"Returns a list of segments to align with the waveform generated by the collect chunks function.\"\"\"\n",
    "    chunks = []\n",
    "    current_frame = 0\n",
    "    for entry in segments:\n",
    "        speech_length = entry['end'] - entry['start']\n",
    "        end_frame = current_frame + speech_length\n",
    "        chunks.append(\n",
    "            {'start': current_frame,\n",
    "             'end': end_frame}\n",
    "            )\n",
    "        current_frame = end_frame\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_chunks(tss: List[dict],\n",
    "                wav: torch.Tensor):\n",
    "    chunks = []\n",
    "    cur_start = 0\n",
    "    for i in tss:\n",
    "        chunks.append((wav[cur_start: i['start']]))\n",
    "        cur_start = i['end']\n",
    "    return torch.cat(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_and_align_chunks(wav: torch.Tensor, segments: List[dict]):\n",
    "    new_wav = collect_chunks(wav, segments)\n",
    "    new_segs = align_chunks(segments)\n",
    "    return new_wav, new_segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "collect_and_align_chunks() takes 2 positional arguments but 29 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_25364\\1598651359.py\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mwav2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msegs2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcollect_and_align_chunks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mget_vad_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexample_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: collect_and_align_chunks() takes 2 positional arguments but 29 were given"
     ]
    }
   ],
   "source": [
    "wav2, segs2 = collect_and_align_chunks(*get_vad_data(example_file))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Audio to AVR data with cacheing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_avr_data(audio, verbose=False):\n",
    "    avr = AVR()\n",
    "    results = avr.transcribe(audio, verbose=verbose)\n",
    "    return results['segments']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_avr_data(path, waveform=None, refresh=False):\n",
    "    json_path = f\"{path}_avr.json\"\n",
    "    if os.path.exists(json_path) and not refresh:\n",
    "        avr_data = load_json(json_path)\n",
    "    else:\n",
    "        if waveform==None:\n",
    "            waveform = get_waveform(path)\n",
    "        wav = collect_chunks(waveform, get_vad_data(path, waveform=waveform))\n",
    "        print('Transcribing AVR data')\n",
    "        avr_data = new_avr_data(wav)\n",
    "        cache_json(avr_data, json_path)\n",
    "        print(f\"Done. Data saved: '{json_path}'\")\n",
    "    return avr_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing AVR data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4494/4494 [00:09<00:00, 462.13frames/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done. Data saved: './data/en_example.wav_avr.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "avr_results = get_avr_data(example_file, refresh=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" and says, how do I get to Dublin? And the answer that comes back is, well, I wouldn't start from here, Sonny. That is to say, much of political philosophy develops theories that take no account of where we actually are and how the theories that people argue about in the journals and in the literature actually could be implemented in the world, if at all. And this spills over into normative arguments made by other scholars. Thomas Piketty in his book, Capital in the 21st Century, argues for a 4% global wealth tax. Well, good luck with that. Who's gonna implement a 4% global wealth tax? So when I think about normative questions...\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_full_text(avr_data):\n",
    "    return ''.join([segment['text'] for segment in avr_data])\n",
    "\n",
    "get_full_text(avr_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "050044240cf91f37d85165fce9cd1b29ae697f4786d26880d31f6038c78120fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
