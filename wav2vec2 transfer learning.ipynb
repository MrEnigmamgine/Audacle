{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display, Audio\n",
    "import torch, torchaudio\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor, TrainingArguments, Trainer\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 16_000\n",
    "folder = './data/jotun 02-05-2023/'\n",
    "output_dir = './training_data/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook I will be attempting to teach wav2vec2 to recognize the voices of myself and my players."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intention here is to generate a torch dataset object with 5 minutes of samples longer than 1s for each speaker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>length</th>\n",
       "      <th>speaker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3249696</td>\n",
       "      <td>3299296</td>\n",
       "      <td>49600</td>\n",
       "      <td>JadePixie_7138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4489760</td>\n",
       "      <td>4544992</td>\n",
       "      <td>55232</td>\n",
       "      <td>JadePixie_7138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5837856</td>\n",
       "      <td>5864928</td>\n",
       "      <td>27072</td>\n",
       "      <td>JadePixie_7138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>5873696</td>\n",
       "      <td>5899744</td>\n",
       "      <td>26048</td>\n",
       "      <td>JadePixie_7138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5901344</td>\n",
       "      <td>5941216</td>\n",
       "      <td>39872</td>\n",
       "      <td>JadePixie_7138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1489</th>\n",
       "      <td>193191968</td>\n",
       "      <td>193212384</td>\n",
       "      <td>20416</td>\n",
       "      <td>Crux_4429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1490</th>\n",
       "      <td>193484320</td>\n",
       "      <td>193596896</td>\n",
       "      <td>112576</td>\n",
       "      <td>Crux_4429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1491</th>\n",
       "      <td>194253344</td>\n",
       "      <td>194291680</td>\n",
       "      <td>38336</td>\n",
       "      <td>Crux_4429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1492</th>\n",
       "      <td>194312224</td>\n",
       "      <td>194329056</td>\n",
       "      <td>16832</td>\n",
       "      <td>Crux_4429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1493</th>\n",
       "      <td>194343968</td>\n",
       "      <td>194360288</td>\n",
       "      <td>16320</td>\n",
       "      <td>Crux_4429</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2734 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          start        end  length         speaker\n",
       "2       3249696    3299296   49600  JadePixie_7138\n",
       "11      4489760    4544992   55232  JadePixie_7138\n",
       "12      5837856    5864928   27072  JadePixie_7138\n",
       "13      5873696    5899744   26048  JadePixie_7138\n",
       "14      5901344    5941216   39872  JadePixie_7138\n",
       "...         ...        ...     ...             ...\n",
       "1489  193191968  193212384   20416       Crux_4429\n",
       "1490  193484320  193596896  112576       Crux_4429\n",
       "1491  194253344  194291680   38336       Crux_4429\n",
       "1492  194312224  194329056   16832       Crux_4429\n",
       "1493  194343968  194360288   16320       Crux_4429\n",
       "\n",
       "[2734 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = utils.audacity.get_audacity_object('./data/jotun 02-05-2023/')\n",
    "\n",
    "dfs = []\n",
    "for file in project['files']:\n",
    "    filename = file['filename']\n",
    "    speakerName = utils.get_discord_name(filename)\n",
    "    targetDir = os.path.join(output_dir, speakerName)\n",
    "    os.makedirs(targetDir, exist_ok=True)\n",
    "    \n",
    "    fullPath = os.path.join(project['data_path'], filename)\n",
    "    vadData = utils.prepare.get_vad_data(fullPath)\n",
    "    df = pd.DataFrame(vadData)\n",
    "    df['length'] = df['end'] - df['start']\n",
    "    df['speaker'] = speakerName\n",
    "    df = df[df['length'] > SR]\n",
    "    dfs.append(df)\n",
    "\n",
    "segments_df = pd.concat(dfs)\n",
    "segments_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CS12_4510           954\n",
       "Crux_4429           781\n",
       "IlMaximuslI_9218    427\n",
       "JadePixie_7138      291\n",
       "Marburg42_5566      281\n",
       "Name: speaker, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments_df.speaker.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "speaker\n",
       "CS12_4510           3365.272\n",
       "Crux_4429           1710.092\n",
       "IlMaximuslI_9218     953.300\n",
       "JadePixie_7138       658.132\n",
       "Marburg42_5566       645.564\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments_df.groupby('speaker').length.sum() / SR"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-base-superb-sid\")\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\"superb/wav2vec2-base-superb-sid\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "args = TrainingArguments(\n",
    "    f\"wav2vec2-base-superb-sid-finetuned-ks\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset= ...,\n",
    "    eval_dataset= ...,\n",
    "    tokenizer= feature_extractor,\n",
    "    compute_metrics= ...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "load_metric('accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 1.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_metric = load_metric(\"accuracy\")\n",
    "results = accuracy_metric.compute(references=[0, 1, 2, 0, 1, 2], predictions=[0, 1, 2, 0, 1, 2], )\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
