{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "print('Hello World')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/crux/anaconda3/envs/audacle/bin/python\n",
      "/home/crux/anaconda3/envs/audacle/bin/pip\n"
     ]
    }
   ],
   "source": [
    "!which python\n",
    "!which pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SR = 16000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ffmpeg\n",
    "\n",
    "def load_audio(file: str, sr: int = SR):\n",
    "    \"\"\"\n",
    "    Open an audio file and read as mono waveform, resampling as necessary\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file: str\n",
    "        The audio file to open\n",
    "\n",
    "    sr: int\n",
    "        The sample rate to resample the audio if necessary\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A NumPy array containing the audio waveform, in float32 dtype.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # This launches a subprocess to decode audio while down-mixing and resampling as necessary.\n",
    "        # Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\n",
    "        out, _ = (\n",
    "            ffmpeg.input(file, threads=0)\n",
    "            .output(\"-\", format=\"s16le\", acodec=\"pcm_s16le\", ac=1, ar=sr)\n",
    "            .run(cmd=[\"ffmpeg\", \"-nostdin\"], capture_stdout=True, capture_stderr=True)\n",
    "        )\n",
    "    except ffmpeg.Error as e:\n",
    "        raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n",
    "\n",
    "    return np.frombuffer(out, np.int16).flatten().astype(np.float32) / 32768.0  #Why 32768?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting a script into chunks small enough to be sent to chat-gpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "print(tokenizer(\"Hello world\")['input_ids'])\n",
    "print(tokenizer(\" Hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "file = '../data/jotun 02-05-2023/script.txt'\n",
    "\n",
    "with open(file, 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "chunks = text.split('\\n\\n')\n",
    "tsizes = [len(tokenizer(chunk)['input_ids']) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = []\n",
    "bag = []\n",
    "total_tokens = 0\n",
    "max_size = 3600\n",
    "for chunk, tsize in zip(chunks, tsizes):\n",
    "    if total_tokens + tsize > max_size:\n",
    "        posts.append(bag)\n",
    "        bag = []\n",
    "        total_tokens = 0\n",
    "    total_tokens += tsize\n",
    "    bag.append(chunk)\n",
    "else:\n",
    "    posts.append(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, post in enumerate(posts):\n",
    "    print(len(post))\n",
    "    post_text = '\\n\\n'.join(post)\n",
    "\n",
    "    # with open(f\"./posts/post_{i}.txt\", 'w') as f:\n",
    "    #     f.write(post_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching models with HF transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import os\n",
    "\n",
    "pipelines_dir = './pretrained_pipelines'\n",
    "model_repo=\"openai/whisper-medium.en\"\n",
    "\n",
    "\n",
    "whisper_asr = transformers.pipeline(model=model_repo)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "savedir = os.path.join(pipelines_dir, model_repo)\n",
    "whisper_asr.save_pretrained(savedir)\n",
    "del whisper_asr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = 'automatic-speech-recognition'\n",
    "whisper_asr = transformers.pipeline(model=savedir, task=task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crux/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ' We all somehow lived Yeah is a pretty pretty amazing thing We got very lucky Apparently and I guess unintentionally very well-prepared'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_asr.predict('only_speech.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav = load_audio('only_speech.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/crux/.local/lib/python3.10/site-packages/transformers/generation/utils.py:1273: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 448 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ' We all somehow lived Yeah is a pretty pretty amazing thing We got very lucky Apparently and I guess unintentionally very well-prepared'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whisper_asr.predict(wav)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audacle",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bba165198c58d21cab5c96fc499a67a319836f1c267b00169c413e11ceb73b97"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
