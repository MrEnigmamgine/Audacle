{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a script\n",
    "\n",
    "One of the things that dissapoints me most about OpenAI's Whisper is that the resulting text is just one long wall of text.  Fortunately, we are using a discord bot called Craig to record our DnD sessions.  With Craig, each person's voice is recorded to their own audio file, which means that the audio is already diarized.\n",
    "\n",
    "Therefore it stands to reason that in order to generate a text file in a script format, all I would need to do is process each file individually then order the segments in chronological order.  Each time the speaker changes simply insert a line with that person's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import whisper\n",
    "import torch\n",
    "import torchaudio as ta\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "from IPython.display import Audio, display\n",
    "\n",
    "print(whisper.__version__)\n",
    "print(ta.__version__)\n",
    "print(torch.__version__)\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing Audacity projects\n",
    "The method we've been using with Craig is to download our recordings as an audacity project.  Audacity projects have a `project.aup` file, which is just a text file that tells the program how to arrange the audio files used in the project.  Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/db_02-03-2023/'\n",
    "aup_file = 'V8DcTChKFF.aup'\n",
    "\n",
    "with open(os.path.join(data_dir, aup_file)) as f:\n",
    "    aup = BeautifulSoup(f, features=\"lxml-xml\")\n",
    "\n",
    "# Project metadata\n",
    "project = aup.find('project')\n",
    "pprint(project.attrs)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`projname` tells us the directory in which the audio files are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_data_dir = project.get('projname')\n",
    "\n",
    "proj_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = os.path.join(data_dir, proj_data_dir)\n",
    "os.listdir(p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 audio files and two data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proj_imports = project.find_all('import')\n",
    "proj_imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here there's two attributes that seem immediately useful.  `filename` tells us where the audio is.  `offset` tells us how each file aligns with the others in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discord_name_pattern = r'.+-(.*)\\..+'\n",
    "proj_files = []\n",
    "\n",
    "for item in proj_imports:\n",
    "    filename, offset= item.get('filename'), float(item.get('offset'))\n",
    "    username = re.search(discord_name_pattern, filename).group(1)\n",
    "    out = {\n",
    "        'filename': filename,\n",
    "        'username': username,\n",
    "        'offset': offset\n",
    "    }\n",
    "    proj_files.append(out)\n",
    "\n",
    "pprint(proj_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whisper_dir = '../models/whisper'\n",
    "transcriber =   whisper.load_model('small.en', download_root= whisper_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for part in proj_files[:1]:\n",
    "    filename = part.get('filename')\n",
    "    username = part.get('username')\n",
    "    offset = part.get('offset')\n",
    "\n",
    "    path = os.path.join(data_dir, proj_data_dir, filename)\n",
    "    wav = whisper.load_audio(path)\n",
    "\n",
    "wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition_on_previous_text=False made this run in 12 minutes.  Without the argument it took 21 minutes.\n",
    "results = transcriber.transcribe(wav, condition_on_previous_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thing in results['segments']:\n",
    "    print(f\"[{thing['start']} - {thing['end']}] {thing['text']}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm.. so Whisper is generating a lot of repetitive text.  This is likely due to the fact that the audio is very long with long sections of silence.  However, I'm noticing that the timestamps presented are much more accurate than normal.  Likely another benifit of setting `condition_on_previous_text=False`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm wondering if using Siler VAD to strip out the non-speech will improve transcription accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silero_dir = '../models/silero-vad'\n",
    "vad, utils =    torch.hub.load(repo_or_dir=silero_dir,\n",
    "                               source='local',\n",
    "                               model='silero_vad',\n",
    "                               force_reload=True,\n",
    "                               onnx=False)\n",
    "\n",
    "(get_speech_timestamps,\n",
    " save_audio,\n",
    " read_audio,\n",
    " VADIterator,\n",
    " collect_chunks) = utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_timestamps = get_speech_timestamps(wav, vad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speech_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLING_RATE = 16000\n",
    "speech_only = collect_chunks(speech_timestamps, torch.tensor(wav))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Audio(data=speech_only, rate=SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6m 46s!\n",
    "results = transcriber.transcribe(speech_only, condition_on_previous_text=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for thing in results['segments']:\n",
    "    print(f\"[{thing['start']} - {thing['end']}] {thing['text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results['segments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for segment in results['segments']:\n",
    "    segment['start_frame'] = segment['start'] * SAMPLING_RATE\n",
    "    segment['end_frame'] = segment['end'] * SAMPLING_RATE\n",
    "    segment['length'] = segment['end_frame'] - segment['start_frame']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['segments'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_ts_df = pd.DataFrame(speech_timestamps)\n",
    "speech_ts_df['length'] = speech_ts_df['end'] - speech_ts_df['start']\n",
    "speech_ts_df.sort_values(by='start')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_timestamps[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(speech_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_timestamps = []\n",
    "current_frame = 0\n",
    "for i, entry in enumerate(speech_timestamps):\n",
    "    speech_length = entry['end'] - entry['start']\n",
    "    end_frame = current_frame + speech_length\n",
    "    chunk_timestamps.append(\n",
    "        {'start': current_frame,\n",
    "         'end': end_frame\n",
    "        }\n",
    "        )\n",
    "    current_frame = end_frame+1\n",
    "\n",
    "len(chunk_timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 1\n",
    "b = speech_timestamps[a]\n",
    "c = chunk_timestamps[a]\n",
    "d,e = b['start'], b['end']\n",
    "f,g = c['start'], c['end']\n",
    "\n",
    "display(Audio(wav[d:e], rate=SAMPLING_RATE))\n",
    "display(Audio(speech_only[f:g], rate=SAMPLING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=450\n",
    "i = results['segments'][a]\n",
    "start = round(i['start'] * 16000)\n",
    "end = round(i['end'] * 16000)\n",
    "print(start, i['text'], end)\n",
    "display(Audio(speech_only[start:end], rate=SAMPLING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results['segments'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time to use pandas\n",
    "\n",
    "The timestamps provided by Whisper become slightly out of sync with the timestamps provided by Silero.  This means that I can't match Whisper segments directly with Silero segments.  \n",
    "However, the drift doesn't seem too bad and is likely caused by the way that Whisper is converting the frame timestamps to seconds, so it should be sufficient to just find the closest ones.\n",
    "\n",
    "By putting both the long waveform timestamps and the truncated chunk timestamps into the same dataframe I can easily match the two together with a `segment_id` identifying them as the same segment of audio.  Additionally, having the timestamps sorted will make it easier to match Silero segments with Whisper segments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_df = pd.DataFrame(speech_timestamps)\n",
    "vad_df['length'] = vad_df['end'] - vad_df['start']\n",
    "vad_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdf = pd.DataFrame(chunk_timestamps)\n",
    "tdf.columns = ['chunk_start','chunk_end']\n",
    "tdf['chunk_length'] = tdf['chunk_end'] - tdf['chunk_start']\n",
    "tdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_df = pd.concat([vad_df, tdf], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vad_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_df = pd.DataFrame(results['segments'])\n",
    "w_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "050044240cf91f37d85165fce9cd1b29ae697f4786d26880d31f6038c78120fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
